---
title: WDAqua@ESWC2018
summary: ESWC 2018 took place from the 3rd to the 7th of June 2018. This post is an account of the contributions made by WDAqua.
---

Like every early summer from 2004, Semantic Web researchers and amateurs have gathered at [ESWC](https://2018.eswc-conferences.org/) (Extended Semantic Web Conference), this time in Heraklion, Crete.
WDAqua has been one of the protagonists of this edition, with presentations, tutorials, workshops, and several papers published in the various tracks of the conference.

Workshops and tutorials were scheduled on the first two days of the conference. On the first, WDAqua was present with [QAtutorial](http://qatutorial.sda.tech/). [Jens Lehmann](http://jens-lehmann.org/), [Andreas Both](http://wdaqua.eu/supervisors/andreas-both/), [Ioanna Lytra](http://wdaqua.eu/supervisors/ioanna-lytra/), [Denis Lukovnikov](http://wdaqua.eu/students/denis-lukovnikov/), and [Kuldeep Singh](http://wdaqua.eu/students/kuldeep-singh/) were among the organisers of this tutorial, in which participants could get a hands-on experience of two types of QA systems: semantic parsing– and deep learning–based. Regarding the first, the Qanary framework was presented, explaining attendees how that works and how to use it. Concerning the second type of approaches, the tutorial involved a basic introduction on how to apply neural networks algorithms, preparing training and test datasets, and explaining how to implement and evaluate the model.

The first day of the main conference was dedicated to the presentation of the accepted papers, but also to presenting the challenges: tasks involving the use of semantic in data, that participants must carried out using innovative approach possibly improving the state of the art. This year Semantic Web Challenges were an official track of the conference: they were submitted in a paper format and peer-reviewed by other researchers. 
One of the challenges was particularly relevant for the topic of our project: the [SQA Challenge](https://project-hobbit.eu/challenges/sqa-challenge-eswc-2018/) focused on the creation of fast and efficient QA systems. In more technical words, the task was defined as “given an RDF dataset and and a large volume of natural language questions or keywords, return the correct answers (or SPARQL queries that retrieves those answers)” (see the [link of the challenge](https://project-hobbit.eu/challenges/sqa-challenge-eswc-2018/)). The WDAqua people could not miss that: [Dennis Diefenbach](http://wdaqua.eu/students/dennis-diefenbach/). [Kamal Singh](http://wdaqua.eu/supervisors/kamal-singh/), and [Pierre Maret](http://wdaqua.eu/supervisors/pierre-maret/) participated with [WDAqua-core1](https://project-hobbit.eu/wp-content/uploads/2018/05/SQA_Paper_1.pdf), a QA system that is one of the main outcome of our project, and won! Their approach gains in terms of scalability from the use of highly efficient tools in its pipeline, such as Lucene and HDT, and the reduction of the HTTP requests overhead. Its performance is comparable to the best QA systems available.

In the Resources track, [Kuldeep Singh](http://wdaqua.eu/students/kuldeep-singh/) and [Andreas Both](http://wdaqua.eu/supervisors/andreas-both/) were among the authors of [*Frankenstein - a Platform Enabling Reuse of Question Answering Components*](https://link.springer.com/chapter/10.1007%2F978-3-319-93417-4_40). This paper presents the components made available within the Frankenstein framework, which allow to build QA systems that are modular and adaptable to the query they process. 

<blockquote class="twitter-tweet tw-align-center" data-lang="en"><p lang="en" dir="ltr">Don&#39;t miss the presentation of our <a href="https://twitter.com/OyeKuldeep?ref_src=twsrc%5Etfw">@OyeKuldeep</a> &amp; Andreas Both today: their paper describes the collection of reusable components within their QA framework Frankenstein. Combining them allows to create 380 QA systems to fit better the user needs! <a href="https://twitter.com/hashtag/eswc2018?src=hash&amp;ref_src=twsrc%5Etfw">#eswc2018</a> <a href="https://twitter.com/EU_H2020?ref_src=twsrc%5Etfw">@EU_H2020</a></p>&mdash; WDAqua (@WDAqua) <a href="https://twitter.com/WDAqua/status/1003909290153766913?ref_src=twsrc%5Etfw">June 5, 2018</a></blockquote>
<script async src="https://platform.twitter.com/widgets.js" charset="utf-8"></script>


On the second day of the conference, [Lucie-Aimée Kaffee](http://wdaqua.eu/students/lucie-aim%C3%A9e-kaffee/) presented her research track paper [*Mind the (Language) Gap- Generation of Multilingual Wikipedia Summaries from Wikidata for ArticlePlaceholders*](https://link.springer.com/chapter/10.1007%2F978-3-319-93417-4_21), authored together with [Hady Elsahar](http://wdaqua.eu/students/hady-el-sahar/), [Pavlos Vougiouklis](http://wdaqua.eu/students/pavlos-vougiouklis/), [Christophe Gravier](http://wdaqua.eu/supervisors/christophe-gravier/), (Frederique Laforest)[http://wdaqua.eu/supervisors/fr%C3%A9d%C3%A9rique-laforest/], Jonathon Hare, and [Elena Simperl](http://wdaqua.eu/supervisors/elena-simperl/). Their work describes their experiment to support editors of Wikipedia underserved languages using text automatically generated from structured data, i.e. triples from Wikidata. Furthermore, they rely on Arabic and Esperanto Wikipedians to demonstrate the quality and the usefulness of their system.

<blockquote class="twitter-tweet tw-align-center" data-lang="en"><p lang="en" dir="ltr">Great talk from <a href="https://twitter.com/frimelle?ref_src=twsrc%5Etfw">@frimelle</a> at <a href="https://twitter.com/eswc_conf?ref_src=twsrc%5Etfw">@eswc_conf</a> on Neural generation of multilingual Wikipedia summaries from WikiData for article placeholders <a href="https://twitter.com/hashtag/eswc2018?src=hash&amp;ref_src=twsrc%5Etfw">#eswc2018</a> <a href="https://twitter.com/hashtag/multilingual?src=hash&amp;ref_src=twsrc%5Etfw">#multilingual</a> <a href="https://twitter.com/hashtag/wikipedia?src=hash&amp;ref_src=twsrc%5Etfw">#wikipedia</a> <a href="https://twitter.com/hashtag/wikidata?src=hash&amp;ref_src=twsrc%5Etfw">#wikidata</a> <a href="https://twitter.com/hashtag/textgeneration?src=hash&amp;ref_src=twsrc%5Etfw">#textgeneration</a> <a href="https://twitter.com/hashtag/languagegap?src=hash&amp;ref_src=twsrc%5Etfw">#languagegap</a> <a href="https://twitter.com/hashtag/teamsoton?src=hash&amp;ref_src=twsrc%5Etfw">#teamsoton</a> <a href="https://t.co/h6TadJ6zxg">pic.twitter.com/h6TadJ6zxg</a></p>&mdash; Samicat (@SamiKanza) <a href="https://twitter.com/SamiKanza/status/1004350236494573568?ref_src=twsrc%5Etfw">June 6, 2018</a></blockquote>
<script async src="https://platform.twitter.com/widgets.js" charset="utf-8"></script>

Finally, on the last day of the conference it was the turn for [Dennis Diefenbach](http://wdaqua.eu/students/dennis-diefenbach/) – who has recently [completed his PhD](https://twitter.com/WDAqua/status/998906321335439360), congratulations Dennis! – to present a piece of research he carried out together with Andreas Thalhammer. Their paper, published in the Resources track, presented two reusable components for RDF knowledge bases: the PageRankRDF and SummaServer. The first ranks RDF triples and is based on the PageRank algorithm. The latter performs entity summarisation. Both components have been implemented in the WDAqua QA pipeline. The title of the paper is *PageRank and Generic Entity Summarization for RDF Knowledge Bases* and you can find it [here](https://link.springer.com/chapter/10.1007%2F978-3-319-93417-4_10).

